{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df942c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nonpawit/Desktop/medicine-scan-lm/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/nonpawit/Desktop/medicine-scan-lm/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "max_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-DRUG_NAME\": 1,\n",
    "    \"I-DRUG_NAME\": 2,\n",
    "    \"B-DOSAGE\": 3,\n",
    "    \"I-DOSAGE\": 4,\n",
    "    \"B-FORM\": 5,\n",
    "    \"I-FORM\": 6,\n",
    "    \"B-WARNINGS\": 7,\n",
    "    \"I-WARNINGS\": 8,\n",
    "    \"B-INDICATIONS\": 9,\n",
    "    \"I-INDICATIONS\": 10,\n",
    "    \"B-USAGE_INSTRUCTIONS\": 11,\n",
    "    \"I-USAGE_INSTRUCTIONS\": 12\n",
    "}\n",
    "\n",
    "def prepare_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        raw_data = json.load(f)\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    labels_list = []\n",
    "    for item in raw_data:\n",
    "        tokens = item['tokens']\n",
    "        labels = item['labels']\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        label_ids = [label2id[label] for label in labels]\n",
    "\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        label_ids = label_ids[:max_length]\n",
    "\n",
    "        pad_len = max_length - len(input_ids)\n",
    "        if pad_len > 0:\n",
    "            input_ids += [tokenizer.pad_token_id] * pad_len\n",
    "            attention_mask += [0] * pad_len\n",
    "            label_ids += [-100] * pad_len\n",
    "\n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        labels_list.append(label_ids)\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attention_mask_list,\n",
    "        \"labels\": labels_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daedb6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        raw_data = json.load(f)\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    labels_list = []\n",
    "    for item in raw_data:\n",
    "        tokens = item['tokens']\n",
    "        labels = item['labels']\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        label_ids = [label2id[label] for label in labels]\n",
    "\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        label_ids = label_ids[:max_length]\n",
    "\n",
    "        pad_len = max_length - len(input_ids)\n",
    "        if pad_len > 0:\n",
    "            input_ids += [tokenizer.pad_token_id] * pad_len\n",
    "            attention_mask += [0] * pad_len\n",
    "            label_ids += [-100] * pad_len\n",
    "\n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        labels_list.append(label_ids)\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attention_mask_list,\n",
    "        \"labels\": labels_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "421bbc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Dataset for Evaluation\n",
    "test_sequence_data = prepare_data('finetuning_data/test.json')\n",
    "test_shuffled_data = prepare_data('shuffled_finetuning_data/test.json')\n",
    "\n",
    "# Prepare the data\n",
    "from datasets import Dataset\n",
    "test_sequence_dataset = Dataset.from_dict(test_sequence_data)\n",
    "test_shuffled_dataset = Dataset.from_dict(test_shuffled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5537312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\n",
    "    \"O\",\n",
    "    \"B-DRUG_NAME\",\n",
    "    \"I-DRUG_NAME\",\n",
    "    \"B-DOSAGE\",\n",
    "    \"I-DOSAGE\",\n",
    "    \"B-FORM\",\n",
    "    \"I-FORM\",\n",
    "    \"B-WARNINGS\",\n",
    "    \"I-WARNINGS\",\n",
    "    \"B-INDICATIONS\",\n",
    "    \"I-INDICATIONS\",\n",
    "    \"B-USAGE_INSTRUCTIONS\",\n",
    "    \"I-USAGE_INSTRUCTIONS\"\n",
    "]\n",
    "\n",
    "id2label = {i: label for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46104ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute metrics\n",
    "import numpy as np\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for pred, label in zip(preds, labels):\n",
    "        current_preds = []\n",
    "        current_labels = []\n",
    "        for p_, l_ in zip(pred, label):\n",
    "            if l_ != -100:\n",
    "                current_preds.append(id2label[p_])\n",
    "                current_labels.append(id2label[l_])\n",
    "        true_predictions.append(current_preds)\n",
    "        true_labels.append(current_labels)\n",
    "\n",
    "    # precision, recall, f1\n",
    "    precision = precision_score(true_labels, true_predictions)\n",
    "    recall = recall_score(true_labels, true_predictions)\n",
    "    f1 = f1_score(true_labels, true_predictions)\n",
    "\n",
    "    # accuracy (custom)\n",
    "    total = sum(len(labels) for labels in true_labels)\n",
    "    correct = sum(\n",
    "        p == l\n",
    "        for preds, labels in zip(true_predictions, true_labels)\n",
    "        for p, l in zip(preds, labels)\n",
    "    )\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "345f8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./ner-roberta-shuffled\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8e5bff",
   "metadata": {},
   "source": [
    "## ðŸ’¬ XLM-ROBERTa-Base (Sequence Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f057806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"token-classification\", model=\"waterondaway/xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed9cd44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "train_data = prepare_data('shuffled_finetuning_data/train.json')\n",
    "eval_data = prepare_data('shuffled_finetuning_data/eval.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba385d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "eval_dataset = Dataset.from_dict(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b829d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./ner-roberta-shuffled\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=pipe,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bcd2e3",
   "metadata": {},
   "source": [
    "## ðŸ’¬ XLM-ROBERTa-Base (Shuffled Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76301f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"token-classification\", model=\"waterondaway/xlm-roberta-base-shuffled\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
