{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer for Fine-Tuning Language Model üíª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/waterondaway/Laboratory/medicine-scan-lm/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# XLM Roberta Base - Tokenizer Download From HuggingFace\n",
    "from transformers import XLMRobertaTokenizer\n",
    "xlm_roberta_tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PythaiNLP - Tokenizer Download From Library\n",
    "from pythainlp.tokenize import word_tokenize as pythainlp_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing ‚öôÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset to tokenizer\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('../data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PATIENT_NAME</th>\n",
       "      <th>PATIENT_ID</th>\n",
       "      <th>PATIENT_BIRTHDATE</th>\n",
       "      <th>DRUG_NAME</th>\n",
       "      <th>DOSAGE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>DRUG_REG_NO</th>\n",
       "      <th>MFG_DATE</th>\n",
       "      <th>EXP_DATE</th>\n",
       "      <th>WARNINGS</th>\n",
       "      <th>INDICATIONS</th>\n",
       "      <th>USAGE_INSTRUCTIONS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡∏ß‡∏£‡∏£‡∏ì‡∏£‡∏±‡∏ï‡∏ô‡πå ‡πÄ‡∏û‡∏µ‡∏¢‡∏¢‡∏≤</td>\n",
       "      <td>7383942134581</td>\n",
       "      <td>12 ‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô 2489</td>\n",
       "      <td>‡πÑ‡∏≠‡∏ö‡∏π‡πÇ‡∏û‡∏£‡πÄ‡∏ü‡∏ô</td>\n",
       "      <td>12.5mg</td>\n",
       "      <td>‡πÄ‡∏°‡πá‡∏î</td>\n",
       "      <td>REG026</td>\n",
       "      <td>2024-10-15</td>\n",
       "      <td>2026-08-18</td>\n",
       "      <td>‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Ç‡∏≠‡∏á‡πÅ‡∏û‡∏ó‡∏¢‡πå</td>\n",
       "      <td>‡∏•‡∏î‡πÑ‡∏Ç‡πâ‡πÉ‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÑ‡∏Ç‡πâ‡∏™‡∏π‡∏á</td>\n",
       "      <td>‡πÄ‡∏Ç‡∏¢‡πà‡∏≤‡∏Ç‡∏ß‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡∏†‡∏≤‡∏ô‡∏∏‡∏û‡∏• ‡∏ô‡∏≤‡∏è‡∏Ñ‡∏≤‡∏¢‡∏µ</td>\n",
       "      <td>5672192371808</td>\n",
       "      <td>12 ‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏° 2501</td>\n",
       "      <td>Diclofenac</td>\n",
       "      <td>500mg</td>\n",
       "      <td>‡πÄ‡∏°‡πá‡∏î</td>\n",
       "      <td>REG063</td>\n",
       "      <td>2024-05-07</td>\n",
       "      <td>2028-01-26</td>\n",
       "      <td>‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏¢‡∏≤‡πÉ‡∏´‡πâ‡∏û‡πâ‡∏ô‡∏°‡∏∑‡∏≠‡πÄ‡∏î‡πá‡∏Å</td>\n",
       "      <td>‡∏•‡∏î‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏à‡πá‡∏ö‡∏õ‡∏ß‡∏î‡∏´‡∏•‡∏±‡∏á‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î</td>\n",
       "      <td>‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô 2 ‡πÄ‡∏°‡πá‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏≠‡∏ô</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‡∏û‡∏¥‡∏°‡∏û‡∏Å‡∏≤‡∏ô‡∏ï‡πå ‡∏ô‡∏≤‡∏°‡πÄ‡∏™‡∏ß‡∏ï‡∏£</td>\n",
       "      <td>5737702286578</td>\n",
       "      <td>10 ‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô 2481</td>\n",
       "      <td>Aspirin</td>\n",
       "      <td>5mg</td>\n",
       "      <td>tablet</td>\n",
       "      <td>REG011</td>\n",
       "      <td>2025-02-16</td>\n",
       "      <td>2027-12-07</td>\n",
       "      <td>‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏ô‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥</td>\n",
       "      <td>‡∏ö‡∏£‡∏£‡πÄ‡∏ó‡∏≤‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏ó‡∏≤‡∏á‡πÄ‡∏î‡∏¥‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£</td>\n",
       "      <td>‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡∏ß‡∏±‡∏ô‡∏•‡∏∞ 1 ‡πÄ‡∏°‡πá‡∏î‡∏´‡∏•‡∏±‡∏á‡∏°‡∏∑‡πâ‡∏≠‡∏Å‡∏•‡∏≤‡∏á‡∏ß‡∏±‡∏ô</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PATIENT_NAME     PATIENT_ID PATIENT_BIRTHDATE   DRUG_NAME  DOSAGE  \\\n",
       "0    ‡∏ß‡∏£‡∏£‡∏ì‡∏£‡∏±‡∏ï‡∏ô‡πå ‡πÄ‡∏û‡∏µ‡∏¢‡∏¢‡∏≤  7383942134581  12 ‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô 2489  ‡πÑ‡∏≠‡∏ö‡∏π‡πÇ‡∏û‡∏£‡πÄ‡∏ü‡∏ô  12.5mg   \n",
       "1      ‡∏†‡∏≤‡∏ô‡∏∏‡∏û‡∏• ‡∏ô‡∏≤‡∏è‡∏Ñ‡∏≤‡∏¢‡∏µ  5672192371808   12 ‡∏™‡∏¥‡∏á‡∏´‡∏≤‡∏Ñ‡∏° 2501  Diclofenac   500mg   \n",
       "2  ‡∏û‡∏¥‡∏°‡∏û‡∏Å‡∏≤‡∏ô‡∏ï‡πå ‡∏ô‡∏≤‡∏°‡πÄ‡∏™‡∏ß‡∏ï‡∏£  5737702286578    10 ‡πÄ‡∏°‡∏©‡∏≤‡∏¢‡∏ô 2481     Aspirin     5mg   \n",
       "\n",
       "     FORM DRUG_REG_NO    MFG_DATE    EXP_DATE                  WARNINGS  \\\n",
       "0    ‡πÄ‡∏°‡πá‡∏î      REG026  2024-10-15  2026-08-18  ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Ç‡∏≠‡∏á‡πÅ‡∏û‡∏ó‡∏¢‡πå   \n",
       "1    ‡πÄ‡∏°‡πá‡∏î      REG063  2024-05-07  2028-01-26    ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏¢‡∏≤‡πÉ‡∏´‡πâ‡∏û‡πâ‡∏ô‡∏°‡∏∑‡∏≠‡πÄ‡∏î‡πá‡∏Å   \n",
       "2  tablet      REG011  2025-02-16  2027-12-07      ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏ô‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥   \n",
       "\n",
       "                             INDICATIONS                    USAGE_INSTRUCTIONS  \n",
       "0                   ‡∏•‡∏î‡πÑ‡∏Ç‡πâ‡πÉ‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÑ‡∏Ç‡πâ‡∏™‡∏π‡∏á        ‡πÄ‡∏Ç‡∏¢‡πà‡∏≤‡∏Ç‡∏ß‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á  \n",
       "1               ‡∏•‡∏î‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏à‡πá‡∏ö‡∏õ‡∏ß‡∏î‡∏´‡∏•‡∏±‡∏á‡∏ú‡πà‡∏≤‡∏ï‡∏±‡∏î               ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô 2 ‡πÄ‡∏°‡πá‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏≠‡∏ô  \n",
       "2  ‡∏ö‡∏£‡∏£‡πÄ‡∏ó‡∏≤‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏ó‡∏≤‡∏á‡πÄ‡∏î‡∏¥‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£  ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡∏ß‡∏±‡∏ô‡∏•‡∏∞ 1 ‡πÄ‡∏°‡πá‡∏î‡∏´‡∏•‡∏±‡∏á‡∏°‡∏∑‡πâ‡∏≠‡∏Å‡∏•‡∏≤‡∏á‡∏ß‡∏±‡∏ô  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display sample of dataset\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automate tag labels for XLM Roberta Tokenizer üòà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to automate tag labels for each data\n",
    "def automate_ner_tags_xlm_roberta_tokenizer(sentence):\n",
    "    patient_name_tokens = xlm_roberta_tokenizer.tokenize(str(sentence[\"PATIENT_NAME\"]))\n",
    "    patient_id_tokens = xlm_roberta_tokenizer.tokenize(str(sentence[\"PATIENT_ID\"]))\n",
    "    patient_birthdate_tokens = xlm_roberta_tokenizer.tokenize(str(sentence[\"PATIENT_BIRTHDATE\"]))\n",
    "    drug_name_tokens = xlm_roberta_tokenizer.tokenize(sentence[\"DRUG_NAME\"])\n",
    "    dosage_tokens = xlm_roberta_tokenizer.tokenize(sentence[\"DOSAGE\"])\n",
    "    form_tokens = xlm_roberta_tokenizer.tokenize(sentence[\"FORM\"])\n",
    "    drug_reg_no_tokens = xlm_roberta_tokenizer.tokenize(sentence[\"DRUG_REG_NO\"])\n",
    "    mfg_date_tokens = xlm_roberta_tokenizer.tokenize(sentence[\"MFG_DATE\"])\n",
    "    exp_date_tokens = xlm_roberta_tokenizer.tokenize(sentence[\"EXP_DATE\"])\n",
    "    warnings_tokens = xlm_roberta_tokenizer.tokenize(sentence[\"WARNINGS\"])\n",
    "    indications_tokens = xlm_roberta_tokenizer.tokenize(sentence[\"INDICATIONS\"])\n",
    "    usage_instructions_tokens = xlm_roberta_tokenizer.tokenize(sentence[\"USAGE_INSTRUCTIONS\"])\n",
    "    \n",
    "    patient_name_labels = [\"O\"] * (len(patient_name_tokens)-1)\n",
    "    patient_id_labels = [\"O\"] * (len(patient_id_tokens)-1)\n",
    "    patient_birthdate_labels = [\"O\"] * (len(patient_birthdate_tokens)-1)\n",
    "    drug_name_labels = [\"B-DRUG_NAME\"] + [\"I-DRUG_NAME\"] * (len(drug_name_tokens)-1)\n",
    "    dosage_labels = [\"B-DOSAGE\"] + [\"I-DOSAGE\"] * (len(dosage_tokens) - 1)\n",
    "    form_labels = [\"B-FORM\"] + [\"I-FORM\"] * (len(form_tokens) - 1)\n",
    "    drug_reg_no_labels = [\"B-DRUG_REG_NO\"] + [\"I-DRUG_REG_NO\"] * (len(drug_reg_no_tokens) - 1)\n",
    "    mfg_date_labels = [\"B-MFG_DATE\"] + [\"I-MFG_DATE\"] * (len(mfg_date_tokens)-1)\n",
    "    exp_date_labels = [\"B-EXP_DATE\"] + [\"I-EXP_DATE\"] * (len(exp_date_tokens)-1)\n",
    "    warnings_labels = [\"B-WARNINGS\"] + [\"I-WARNINGS\"] * (len(warnings_tokens)-1)\n",
    "    indications_labels = [\"B-INDICATIONS\"] + [\"I-INDICATIONS\"] * (len(indications_tokens)-1)\n",
    "    usage_instructions_labels = [\"B-USAGE_INSTRUCTIONS\"] + [\"I-USAGE_INSTRUCTIONS\"] * (len(usage_instructions_tokens)-1)\n",
    "    \n",
    "    tokens = patient_name_tokens + patient_id_tokens + patient_birthdate_tokens + drug_name_tokens + dosage_tokens + form_tokens + drug_reg_no_tokens + mfg_date_tokens + exp_date_tokens + warnings_tokens + indications_tokens + usage_instructions_tokens\n",
    "    ner_tags = patient_name_labels + patient_id_labels + patient_birthdate_labels + drug_name_labels + dosage_labels + form_labels + drug_reg_no_labels + mfg_date_labels + exp_date_labels + warnings_labels + indications_labels + usage_instructions_labels\n",
    "    \n",
    "    return {\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": ner_tags\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling function \n",
    "xlm_roberta_tokenizer_formatted_dataset = [automate_ner_tags_xlm_roberta_tokenizer(sentence) for index, sentence in dataset.iterrows()]\n",
    "\n",
    "# Save with JSON File\n",
    "import json\n",
    "with open('../data/xlm_roberta_tokenizer_format.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(xlm_roberta_tokenizer_formatted_dataset, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automate tag labels for PythaiNLP Tokenizer üòà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to automate tag labels for each data\n",
    "def automate_ner_tags_pythainlp_tokenizer(sentence, engine_setup):\n",
    "    patient_name_tokens = pythainlp_tokenizer(sentence[\"PATIENT_NAME\"], engine=engine_setup)\n",
    "    patient_id_tokens = pythainlp_tokenizer(sentence[\"PATIENT_ID\"], engine=engine_setup)\n",
    "    patient_birthdate_tokens = pythainlp_tokenizer(sentence[\"PATIENT_BIRTHDATE\"], engine=engine_setup)\n",
    "    drug_name_tokens = pythainlp_tokenizer(sentence[\"DRUG_NAME\"], engine=engine_setup)\n",
    "    dosage_tokens = pythainlp_tokenizer(sentence[\"DOSAGE\"], engine=engine_setup)\n",
    "    form_tokens = pythainlp_tokenizer(sentence[\"FORM\"], engine=engine_setup)\n",
    "    drug_reg_no_tokens = pythainlp_tokenizer(sentence[\"DRUG_REG_NO\"], engine=engine_setup)\n",
    "    mfg_date_tokens = pythainlp_tokenizer(sentence[\"MFG_DATE\"], engine=engine_setup)\n",
    "    exp_date_tokens = pythainlp_tokenizer(sentence[\"EXP_DATE\"], engine=engine_setup)\n",
    "    warnings_tokens = pythainlp_tokenizer(sentence[\"WARNINGS\"], engine=engine_setup)\n",
    "    indications_tokens = pythainlp_tokenizer(sentence[\"INDICATIONS\"], engine=engine_setup)\n",
    "    usage_instructions_tokens = pythainlp_tokenizer(sentence[\"USAGE_INSTRUCTIONS\"], engine=engine_setup)\n",
    "    \n",
    "    patient_name_labels = [\"O\"] * (len(patient_name_tokens)-1)\n",
    "    patient_id_labels = [\"O\"] * (len(patient_id_tokens)-1)\n",
    "    patient_birthdate_labels = [\"O\"] * (len(patient_birthdate_tokens)-1)\n",
    "    drug_name_labels = [\"B-DRUG_NAME\"] + [\"I-DRUG_NAME\"] * (len(drug_name_tokens)-1)\n",
    "    dosage_labels = [\"B-DOSAGE\"] + [\"I-DOSAGE\"] * (len(dosage_tokens) - 1)\n",
    "    form_labels = [\"B-FORM\"] + [\"I-FORM\"] * (len(form_tokens) - 1)\n",
    "    drug_reg_no_labels = [\"B-DRUG_REG_NO\"] + [\"I-DRUG_REG_NO\"] * (len(drug_reg_no_tokens) - 1)\n",
    "    mfg_date_labels = [\"B-MFG_DATE\"] + [\"I-MFG_DATE\"] * (len(mfg_date_tokens)-1)\n",
    "    exp_date_labels = [\"B-EXP_DATE\"] + [\"I-EXP_DATE\"] * (len(exp_date_tokens)-1)\n",
    "    warnings_labels = [\"B-WARNINGS\"] + [\"I-WARNINGS\"] * (len(warnings_tokens)-1)\n",
    "    indications_labels = [\"B-INDICATIONS\"] + [\"I-INDICATIONS\"] * (len(indications_tokens)-1)\n",
    "    usage_instructions_labels = [\"B-USAGE_INSTRUCTIONS\"] + [\"I-USAGE_INSTRUCTIONS\"] * (len(usage_instructions_tokens)-1)\n",
    "    \n",
    "    tokens = patient_name_tokens + patient_id_tokens + patient_birthdate_tokens + drug_name_tokens + dosage_tokens + form_tokens + drug_reg_no_tokens + mfg_date_tokens + exp_date_tokens + warnings_tokens + indications_tokens + usage_instructions_tokens\n",
    "    ner_tags = patient_name_labels + patient_id_labels + patient_birthdate_labels + drug_name_labels + dosage_labels + form_labels + drug_reg_no_labels + mfg_date_labels + exp_date_labels + warnings_labels + indications_labels + usage_instructions_labels\n",
    "    \n",
    "    return {\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": ner_tags\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling function \n",
    "pythainlp_tokenizer_formatted_dataset = [automate_ner_tags_pythainlp_tokenizer(sentence, 'longest') for index, sentence in dataset.iterrows()]\n",
    "\n",
    "# Save with JSON File\n",
    "import json\n",
    "with open('../data/pythainlp_tokenizer_format.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(pythainlp_tokenizer_formatted_dataset, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
