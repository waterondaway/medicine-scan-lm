{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "312254dd",
   "metadata": {},
   "source": [
    "## ðŸ’¬ Tokenizer for Fine-Tuning Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1039eb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nonpawit/Desktop/medicine-scan-lm/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/nonpawit/Desktop/medicine-scan-lm/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "# Checking version of transformers library\n",
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1439174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "318cbc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prescriptions data\n",
    "import pandas as pd\n",
    "dataset_for_train = pd.read_csv(\"finetuning_data/train.csv\")\n",
    "dataset_for_eval = pd.read_csv(\"finetuning_data/eval.csv\")\n",
    "dataset_for_test = pd.read_csv(\"finetuning_data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "babfb33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def shuffle_token_label_chunks(chunks):\n",
    "    random.shuffle(chunks)\n",
    "    tokens = []\n",
    "    ner_tags = []\n",
    "    for chunk_tokens, chunk_labels in chunks:\n",
    "        tokens.extend(chunk_tokens)\n",
    "        ner_tags.extend(chunk_labels)\n",
    "    return tokens, ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc660a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to automate tag labels for each data\n",
    "def automate_ner_tags_xlm_roberta_tokenizer(sentence):\n",
    "    patient_name_tokens = tokenizer.tokenize(str(sentence[\"patient_name\"]))\n",
    "    patient_id_tokens = tokenizer.tokenize(str(sentence[\"patient_id\"]))\n",
    "    patient_birthdate_tokens = tokenizer.tokenize(str(sentence[\"patient_birthdate\"]))\n",
    "    drug_name_tokens = tokenizer.tokenize(sentence[\"drug_name\"])\n",
    "    dosage_tokens = tokenizer.tokenize(sentence[\"dosage\"])\n",
    "    form_tokens = tokenizer.tokenize(sentence[\"form\"])\n",
    "    drug_reg_no_tokens = tokenizer.tokenize(sentence[\"drug_reg_no\"])\n",
    "    mfg_date_tokens = tokenizer.tokenize(sentence[\"mfg_date\"])\n",
    "    exp_date_tokens = tokenizer.tokenize(sentence[\"exp_date\"])\n",
    "    warnings_tokens = tokenizer.tokenize(sentence[\"warnings\"])\n",
    "    indications_tokens = tokenizer.tokenize(sentence[\"indications\"])\n",
    "    usage_instructions_tokens = tokenizer.tokenize(sentence[\"usage_instructions\"])\n",
    "    \n",
    "    patient_name_labels = [\"O\"] * (len(patient_name_tokens))\n",
    "    patient_id_labels = [\"O\"] * (len(patient_id_tokens))\n",
    "    patient_birthdate_labels = [\"O\"] * (len(patient_birthdate_tokens))\n",
    "    drug_name_labels = [\"B-DRUG_NAME\"] + [\"I-DRUG_NAME\"] * (len(drug_name_tokens)-1)\n",
    "    dosage_labels = [\"B-DOSAGE\"] + [\"I-DOSAGE\"] * (len(dosage_tokens) - 1)\n",
    "    form_labels = [\"B-FORM\"] + [\"I-FORM\"] * (len(form_tokens) - 1)\n",
    "    drug_reg_no_labels = [\"O\"] * (len(drug_reg_no_tokens))\n",
    "    mfg_date_labels = [\"O\"] * (len(mfg_date_tokens))\n",
    "    exp_date_labels = [\"O\"] * (len(exp_date_tokens))\n",
    "    warnings_labels = [\"B-WARNINGS\"] + [\"I-WARNINGS\"] * (len(warnings_tokens)-1)\n",
    "    indications_labels = [\"B-INDICATIONS\"] + [\"I-INDICATIONS\"] * (len(indications_tokens)-1)\n",
    "    usage_instructions_labels = [\"B-USAGE_INSTRUCTIONS\"] + [\"I-USAGE_INSTRUCTIONS\"] * (len(usage_instructions_tokens)-1)\n",
    "    \n",
    "    tokens = patient_name_tokens + patient_id_tokens + patient_birthdate_tokens + drug_name_tokens + dosage_tokens + form_tokens + drug_reg_no_tokens + mfg_date_tokens + exp_date_tokens + warnings_tokens + indications_tokens + usage_instructions_tokens\n",
    "    ner_tags = patient_name_labels + patient_id_labels + patient_birthdate_labels + drug_name_labels + dosage_labels + form_labels + drug_reg_no_labels + mfg_date_labels + exp_date_labels + warnings_labels + indications_labels + usage_instructions_labels\n",
    "\n",
    "    chunks = [\n",
    "        (patient_name_tokens, patient_name_labels),\n",
    "        (patient_id_tokens, patient_id_labels),\n",
    "        (patient_birthdate_tokens, patient_birthdate_labels),\n",
    "        (drug_name_tokens, drug_name_labels),\n",
    "        (dosage_tokens, dosage_labels),\n",
    "        (form_tokens, form_labels),\n",
    "        (drug_reg_no_tokens, drug_reg_no_labels),\n",
    "        (mfg_date_tokens, mfg_date_labels),\n",
    "        (exp_date_tokens, exp_date_labels),\n",
    "        (warnings_tokens, warnings_labels),\n",
    "        (indications_tokens, indications_labels),\n",
    "        (usage_instructions_tokens, usage_instructions_labels),\n",
    "    ]\n",
    "    \n",
    "    # Shuffle the chunks\n",
    "    shuffled_tokens, shuffled_ner_tags = shuffle_token_label_chunks(chunks)\n",
    "    \n",
    "    return {\n",
    "        \"tokens\": shuffled_tokens,\n",
    "        \"labels\": shuffled_ner_tags\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7716298a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function for training and evaluation datasets\n",
    "dataset_for_train = [automate_ner_tags_xlm_roberta_tokenizer(row) for _, row in dataset_for_train.iterrows()]\n",
    "dataset_for_eval = [automate_ner_tags_xlm_roberta_tokenizer(row) for _, row in dataset_for_eval.iterrows()]\n",
    "dataset_for_test = [automate_ner_tags_xlm_roberta_tokenizer(row) for _, row in dataset_for_test.iterrows()]\n",
    "\n",
    "# Save with JSON File\n",
    "import json\n",
    "with open('./shuffled_finetuning_data/train.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(dataset_for_train, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Save with JSON File\n",
    "import json\n",
    "with open('./shuffled_finetuning_data/eval.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(dataset_for_eval, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Save with JSON File\n",
    "with open('./shuffled_finetuning_data/test.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(dataset_for_test, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
